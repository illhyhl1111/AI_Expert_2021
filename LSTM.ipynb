{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/illhyhl1111/AI_Expert_2021/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-HG7MfXo78"
      },
      "source": [
        "# original code from : https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrW_p2qiPiwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2716d8a3-216a-4aa2-e318-bc57c2ad37d4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fec37770ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJptVh0PDO5"
      },
      "source": [
        "# LSTM 예시\n",
        " * input dimension과 output dimension이 3인 LSTMTagger\n",
        " * lstm(input, hidden) 은 해당 input과 주어진 현재 hidden state를 받아,\n",
        "   output과 다음 hidden state를 리턴한다.\n",
        " * lstm(inputs, hidden) 은 해당 input sequence를 주어진 현재 state에 차례로 넣어,\n",
        "   output sequence와 최종 hidden state를 리턴한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slx-CUFqVkKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b994a0bd-f479-4533-d295-d8c1291f7698"
      },
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print(out)\n",
        "    print(hidden)\n",
        "\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.1990, -0.0406, -0.1692]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.1990, -0.0406, -0.1692]]], grad_fn=<StackBackward>), tensor([[[ 0.5052, -0.2285, -0.4953]]], grad_fn=<StackBackward>))\n",
            "tensor([[[ 0.1939, -0.0463,  0.0238]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.1939, -0.0463,  0.0238]]], grad_fn=<StackBackward>), tensor([[[ 0.3800, -0.1626,  0.0492]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.1052, -0.1215,  0.5482]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.1052, -0.1215,  0.5482]]], grad_fn=<StackBackward>), tensor([[[-0.1496, -0.5206,  0.7594]]], grad_fn=<StackBackward>))\n",
            "tensor([[[-0.0193, -0.0928,  0.3600]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.0193, -0.0928,  0.3600]]], grad_fn=<StackBackward>), tensor([[[-0.0415, -0.4691,  0.9442]]], grad_fn=<StackBackward>))\n",
            "tensor([[[ 0.0688, -0.1129,  0.1680]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.0688, -0.1129,  0.1680]]], grad_fn=<StackBackward>), tensor([[[ 0.2030, -0.3624,  0.7017]]], grad_fn=<StackBackward>))\n",
            "tensor([[[ 0.2490, -0.0525,  0.3253]],\n",
            "\n",
            "        [[ 0.1655, -0.0304,  0.3348]],\n",
            "\n",
            "        [[-0.1104, -0.1085,  0.7568]],\n",
            "\n",
            "        [[-0.0148, -0.0855,  0.4162]],\n",
            "\n",
            "        [[ 0.0703, -0.1089,  0.2071]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.0703, -0.1089,  0.2071]]], grad_fn=<StackBackward>), tensor([[[ 0.2099, -0.3541,  0.9947]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNSIBT1HPEuv"
      },
      "source": [
        "# 데이터 예시\n",
        " * 각 단어의 품사를 추론하는 모델을 만드는 것을 목표로 한다.\n",
        " * 각 단어 별로 품사가 labeling된 데이터를 준비한다(아래 코드의 training_data).\n",
        " * 각 단어 혹은 품사 sequence를 숫자로 encoding한다(아래 코드의 prepare_sequence 함수).\n",
        " * 이 때, 단어를 숫자로 encoding할 때는 word_to_ix dictionary를 사용하고, 품사를 숫자로 encoding할 때는 tag_to_ix dictionary를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1fiAxoYVo-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843a4607-b42b-4120-d432-b12e1014c84c"
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYz3ha0nQWV2"
      },
      "source": [
        "# 품사 예측 LSTM 모델\n",
        " * 각 단어를 embedding_dim 차원 공간에 embedding하여, 해당 embedding을 LSTM의 input으로 사용한다.\n",
        " * LSTM은 hidden_dim 차원의 hidden state를 가지며, output인 품사 score는 hidden state에 fully connected layer를 적용하여 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R6nYw_oVtGL"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ogcTXXfRbpJ"
      },
      "source": [
        "# LSTM training\n",
        " * 아까 정의한 prepare_sequence 함수를 이용해 training data를 encoding하여 LSTM에 input으로 넣는다.\n",
        " * LSTM 모델에서 나온 tag score와 tag label와의 NLLLoss를 loss로써 사용하여 학습한다.\n",
        "  (https://pytorch.org/docs/stable/nn.html#nllloss 참조)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmvH2xgWRA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334db613-38aa-4262-9a14-f017fcfd3fcc"
      },
      "source": [
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print('before training \\n : ', tag_scores)\n",
        "    print(tag_scores.max(-1)[1])\n",
        "\n",
        "    inputs = prepare_sequence(training_data[1][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print('before training \\n : ', tag_scores)\n",
        "    print(tag_scores.max(-1)[1])\n",
        "\n",
        "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
        "    # for word i. The predicted tag is the maximum scoring tag.\n",
        "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "    # since 0 is index of the maximum value of row 1,\n",
        "    # 1 is the index of maximum value of row 2, etc.\n",
        "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "    print('after training \\n : ', tag_scores)\n",
        "    print(tag_scores.max(-1)[1])\n",
        "\n",
        "    inputs = prepare_sequence(training_data[1][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print('after training \\n : ', tag_scores)\n",
        "    print(tag_scores.max(-1)[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before training \n",
            " :  tensor([[-0.9339, -0.9493, -1.5142],\n",
            "        [-0.8688, -0.9418, -1.6574],\n",
            "        [-0.9654, -0.9106, -1.5284],\n",
            "        [-0.9343, -0.9188, -1.5696],\n",
            "        [-0.9313, -0.9042, -1.6040]])\n",
            "tensor([0, 0, 1, 1, 1])\n",
            "before training \n",
            " :  tensor([[-0.8420, -1.0027, -1.5981],\n",
            "        [-0.9110, -0.9340, -1.5851],\n",
            "        [-0.9216, -0.9447, -1.5448],\n",
            "        [-0.9435, -0.8967, -1.5953]])\n",
            "tensor([0, 0, 0, 1])\n",
            "after training \n",
            " :  tensor([[-0.0509, -4.0787, -3.4201],\n",
            "        [-5.0753, -0.0176, -4.4908],\n",
            "        [-3.6580, -3.6734, -0.0525],\n",
            "        [-0.0266, -5.2813, -3.8543],\n",
            "        [-4.9028, -0.0197, -4.4132]])\n",
            "tensor([0, 1, 2, 0, 1])\n",
            "after training \n",
            " :  tensor([[-5.0944, -0.0167, -4.5643],\n",
            "        [-4.2834, -4.6038, -0.0241],\n",
            "        [-0.0213, -5.5238, -4.0722],\n",
            "        [-5.4522, -0.0129, -4.7693]])\n",
            "tensor([1, 2, 0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPej6rp3S9PR"
      },
      "source": [
        " * 각 단어에 대해 가장 score가 높은 index에 해당되는 품사가 LSTM이 추론한 품사이다.\n",
        " * 학습한 결과는 'The dog ate the apple'이라는 문장에 대해 'DET NN V DET NN'으로 품사를 추론하고, 'Everybody read that book'이라는 문장에 대해 'NN V DET NN'으로 품사를 추론함을 알 수 있다."
      ]
    }
  ]
}